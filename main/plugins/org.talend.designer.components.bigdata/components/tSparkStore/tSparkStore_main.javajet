<%@ jet
imports="
org.talend.core.model.process.INode
org.talend.core.model.process.ElementParameterParser
org.talend.core.model.metadata.IMetadataTable
org.talend.core.model.metadata.IMetadataColumn
org.talend.core.model.process.IConnection
org.talend.core.model.process.IConnectionCategory
org.talend.core.model.process.EConnectionType
org.talend.designer.codegen.config.CodeGeneratorArgument
org.talend.core.model.metadata.types.JavaTypesManager
org.talend.core.model.metadata.types.JavaType
java.util.List 
java.util.Map
"
%>

<%
	CodeGeneratorArgument codeGenArgument = (CodeGeneratorArgument) argument;
	INode node = (INode)codeGenArgument.getArgument();	
	String cid = node.getUniqueName();
	
	String fieldSeparator = ElementParameterParser.getValue(node, "__FIELD_SEPARATOR_CHAR__");
	boolean rmResultDir = "true".equals(ElementParameterParser.getValue(node, "__RM_OUTPUT__"));

	boolean isStreaming = false;
	boolean isLocalMode = false;
	String storageSource = ElementParameterParser.getValue(node, "__STORAGE_SOURCE__");
	boolean isLocal = "LOCAL".equals(storageSource);
	boolean isHDFS = "HDFS".equals(storageSource);

	String sparkConnection = ElementParameterParser.getValue(node, "__SPARK_CONNECTION__");
	for (INode pNode : node.getProcess().getNodesOfType("tSparkConnection")) {
		if(sparkConnection!=null && sparkConnection.equals(pNode.getUniqueName())) {
			isStreaming = "true".equals(ElementParameterParser.getValue(pNode, "__STREAMING__"));
			isLocalMode = "LOCAL".equals(ElementParameterParser.getValue(pNode, "__SPARK_MODE__"));
		}
	}
	
	if(isHDFS && isLocalMode) {
%>
		if(true) {
			throw new java.lang.UnsupportedOperationException("The HDFS storage mode is not supported in local mode.");
		}
<%
	}

	String previous_node="";
	
	String previousOutputConnectionName = "";
	
	if(node.getIncomingConnections()!=null && node.getIncomingConnections().size()>0) {
		IConnection connection = node.getIncomingConnections().get(0);
		previous_node = connection.getSource().getUniqueName();
		previousOutputConnectionName = connection.getName();
	}
	
	if(!"".equals(previousOutputConnectionName)) {
	
		String contextClass = (isStreaming?"org.apache.spark.streaming.api.java.JavaStreamingContext":"org.apache.spark.api.java.JavaSparkContext");
%>
		<%=contextClass%> ctx_<%=cid%> = (<%=contextClass%>)globalMap.get("<%=ElementParameterParser.getValue(node, "__SPARK_CONNECTION__")%>_SPARK_CONTEXT");
<%

		String namenode = ElementParameterParser.getValue(node, "__FS_DEFAULT_NAME__"); 
		if(isHDFS && !isStreaming) {
%>
			org.apache.hadoop.conf.Configuration conf_<%=cid%> = ctx_<%=cid%>.hadoopConfiguration();
			conf_<%=cid%>.set("fs.default.name", <%=("".equals(namenode)?"\"\"":namenode)%>);
<%
		}
	
		String outputFile = ElementParameterParser.getValue(node, "__OUTPUT_FILENAME__");
		if(isLocal) outputFile = "\"file:///\" + " + outputFile;
		if(isHDFS && isStreaming) outputFile = namenode + "+" + outputFile;
%>	
		String outputFile_<%=cid %> = <%=("".equals(outputFile)?"\"\"":outputFile)%>;
<%
		if (rmResultDir && isLocal) {
%>
            class FileUtils_<%=cid%> {
                public void delete(java.io.File file) {
                    if (file != null && file.exists()) {
                        if (file.isDirectory()) {
                            for (java.io.File f : file.listFiles()) {
                                delete(f);
                            }
                        }
                        file.delete();
                    }
                }
            }

            java.io.File file_<%=cid %> = new java.io.File(<%=ElementParameterParser.getValue(node, "__OUTPUT_FILENAME__")%>);
            FileUtils_<%=cid%> Utils_<%=cid%> = new FileUtils_<%=cid%>();
            Utils_<%=cid%>.delete(file_<%=cid %>);
<%
		}
%>

		if(<%=previous_node%>_<%=previousOutputConnectionName%>_RDD!=null) {
			org.talend.spark.operation.Store.run(outputFile_<%=cid %>, <%=previous_node%>_<%=previousOutputConnectionName%>_RDD, <%=("".equals(fieldSeparator)?"\"\"":fieldSeparator)%>);
		}
<%
	}
%>