<%@ jet
imports="
org.talend.core.model.process.INode
org.talend.core.model.process.ElementParameterParser
org.talend.core.model.metadata.IMetadataTable
org.talend.core.model.metadata.IMetadataColumn
org.talend.core.model.process.IConnection
org.talend.core.model.process.IConnectionCategory
org.talend.core.model.process.EConnectionType
org.talend.designer.codegen.config.CodeGeneratorArgument
org.talend.core.model.metadata.types.JavaTypesManager
org.talend.core.model.metadata.types.JavaType
java.util.List 
java.util.Map
"
%>

<%
	CodeGeneratorArgument codeGenArgument = (CodeGeneratorArgument) argument;
	INode node = (INode)codeGenArgument.getArgument();	
	String cid = node.getUniqueName();
	
	String fieldSeparator = ElementParameterParser.getValue(node, "__FIELD_SEPARATOR_CHAR__");
	boolean rmResultDir = "true".equals(ElementParameterParser.getValue(node, "__RM_OUTPUT__"));

	boolean isStreaming = false;
	boolean isLocalMode = false;
	String storageSource = ElementParameterParser.getValue(node, "__STORAGE_SOURCE__");
	boolean isLocal = "LOCAL".equals(storageSource);
	boolean isHDFS = "HDFS".equals(storageSource);
	boolean isHBase = "HBASE".equals(storageSource);

	String sparkConnection = ElementParameterParser.getValue(node, "__SPARK_CONNECTION__");
	for (INode pNode : node.getProcess().getNodesOfType("tSparkConnection")) {
		if(sparkConnection!=null && sparkConnection.equals(pNode.getUniqueName())) {
			isStreaming = "true".equals(ElementParameterParser.getValue(pNode, "__STREAMING__"));
			isLocalMode = "LOCAL".equals(ElementParameterParser.getValue(pNode, "__SPARK_MODE__"));
		}
	}
	
	if(isHDFS && isLocalMode) {
%>
		if(true) {
			throw new java.lang.UnsupportedOperationException("The HDFS storage mode is not supported in local mode.");
		}
<%
	}

	String previous_node="";
	
	String previousOutputConnectionName = "";
	
	List<IMetadataColumn> columnsMetadataList = null;
	if(node.getIncomingConnections()!=null && node.getIncomingConnections().size()>0) {
		IConnection connection = node.getIncomingConnections().get(0);
		IMetadataTable metadata = connection.getMetadataTable();
		columnsMetadataList = metadata.getListColumns();
		previous_node = connection.getSource().getUniqueName();
		previousOutputConnectionName = connection.getName();
	}
	
	if(!"".equals(previousOutputConnectionName)) {
	
		String contextClass = (isStreaming?"org.apache.spark.streaming.api.java.JavaStreamingContext":"org.apache.spark.api.java.JavaSparkContext");
%>
		<%=contextClass%> ctx_<%=cid%> = (<%=contextClass%>)globalMap.get("<%=ElementParameterParser.getValue(node, "__SPARK_CONNECTION__")%>_SPARK_CONTEXT");
<%
		if(!isHBase) {
			String namenode = ElementParameterParser.getValue(node, "__FS_DEFAULT_NAME__"); 
			if(isHDFS && !isStreaming) {
%>
				org.apache.hadoop.conf.Configuration conf_<%=cid%> = ctx_<%=cid%>.hadoopConfiguration();
				conf_<%=cid%>.set("fs.default.name", <%=("".equals(namenode)?"\"\"":namenode)%>);
<%
			}
	
			String outputFile = ElementParameterParser.getValue(node, "__OUTPUT_FILENAME__");
			if(isLocal) outputFile = "\"file:///\" + " + outputFile;
			if(isHDFS && isStreaming) outputFile = namenode + "+" + outputFile;
%>	
			String outputFile_<%=cid %> = <%=("".equals(outputFile)?"\"\"":outputFile)%>;
<%
			if (rmResultDir && isLocal) {
%>
				java.io.File file_<%=cid %> = new java.io.File(outputFile_<%=cid %>);
			    if(file_<%=cid %>.isDirectory()) {
			        //directory is empty, then delete it
			        if(file_<%=cid %>.list().length==0){
			            file_<%=cid %>.delete();
			        }else{
			            //list all the directory contents
			            String files[] = file_<%=cid %>.list();
			
			            for (String temp : files) {
			                //construct the file_<%=cid %> structure
			                java.io.File fileDelete = new java.io.File(file_<%=cid %>, temp);
			                //recursive delete
			                fileDelete.delete();
			            }
			
			            //check the directory again, if empty then delete it
			            if(file_<%=cid %>.list().length==0){
			                file_<%=cid %>.delete();
			            }
			        }
			    }else{
			        //if file_<%=cid %>, then delete it
			        file_<%=cid %>.delete();
			    }
<%
			}
%>

			if(<%=previous_node%>_<%=previousOutputConnectionName%>_RDD!=null) {
				org.talend.spark.operation.Store.run(outputFile_<%=cid %>, <%=previous_node%>_<%=previousOutputConnectionName%>_RDD, <%=("".equals(fieldSeparator)?"\"\"":fieldSeparator)%>);
			}
<%
		}else {
			String zookeeperHost = ElementParameterParser.getValue(node, "__ZOOKEEPER_HOST_HBASE__");
			String zookeeperPort = (String)ElementParameterParser.getObjectValue(node, "__ZOOKEEPER_PORT_HBASE__");
			String hbaseTable = ElementParameterParser.getValue(node, "__HBASE_TABLE__");
			List<Map<String, String>> mapping = (List<Map<String,String>>)ElementParameterParser.getObjectValue(node,"__MAPPING__");
			java.lang.StringBuilder columns = new java.lang.StringBuilder();
			for(int i=0;i<mapping.size();i++) {
				if(i!=0) columns.append(" + \" \" + ");
				Map<String, String> map = mapping.get(i);
				String family_column= map.get("FAMILY_COLUMN");
				columns.append(family_column);
			}
%>
			java.util.List<Integer> keyList_<%=cid%> = new java.util.ArrayList<Integer>();
<%
			if(columnsMetadataList != null) {
				int index = 0;
				for(IMetadataColumn columnsMetadata : columnsMetadataList) {
					if(columnsMetadata.isKey()) {
%>
						keyList_<%=cid%>.add(<%=index%>);
<%						
					}
					index++;
				}
			}
%>
			org.talend.spark.operation.HBaseStore.run(<%=zookeeperHost%>, <%=zookeeperPort%>, <%=hbaseTable%>, <%=columns%>, <%=previous_node%>_<%=previousOutputConnectionName%>_RDD, keyList_<%=cid%>);
<%
		}
	}
%>