<%@ jet 
	imports="
    org.talend.core.model.process.INode 
    org.talend.core.model.process.EConnectionType
    org.talend.core.model.metadata.IMetadataTable 
	org.talend.core.model.metadata.IMetadataColumn
    org.talend.core.model.process.IConnectionCategory
    org.talend.core.model.process.IConnection
    org.talend.designer.codegen.config.CodeGeneratorArgument
	org.talend.core.model.process.ElementParameterParser
	java.util.Map 
    java.util.List 
	"
%>

<%
	CodeGeneratorArgument codeGenArgument = (CodeGeneratorArgument) argument;
	INode node = (INode)codeGenArgument.getArgument();
	String cid = node.getUniqueName();
	
	String fsDefaultName = ElementParameterParser.getValue(node, "__FS_DEFAULT_NAME__");
	
	boolean useExistingConnection = "true".equals(ElementParameterParser.getValue(node, "__USE_EXISTING_CONNECTION__"));
	List<Map<String, String>> hadoopProps = (List<Map<String,String>>)ElementParameterParser.getObjectValue(node, "__HADOOP_ADVANCED_PROPERTIES__");
		
	String fileName = ElementParameterParser.getValue(node, "__FILENAME__");
	
	//boolean fileAction = "true".equals(ElementParameterParser.getValue(node, "__FILE_ACTION__"));
	String fileAction = ElementParameterParser.getValue(node, "__FILE_ACTION__");
	
	boolean customEncoding="true".equals( ElementParameterParser.getValue(node,"__CUSTOM_ENCODING__"));
	String encoding = ElementParameterParser.getValue(node,"__ENCODING__");
	
	String typeFile = ElementParameterParser.getValue(node,"__TYPEFILE__");
	
	boolean compress = "true".equals(ElementParameterParser.getValue(node, "__COMPRESS__"));
	String compression = ElementParameterParser.getValue(node, "__COMPRESSION__");
	
	boolean isIncludeHeader = ("true").equals(ElementParameterParser.getValue(node,"__INCLUDEHEADER__"));
	
	String fsDefalutName = "fs.default.name";
	
	List<? extends IConnection> inConns = node.getIncomingConnections(EConnectionType.FLOW_MAIN);
	IConnection inConn = null;
	IMetadataTable metadata = null;
	
	if(inConns!=null && inConns.size()> 0) {
		inConn = inConns.get(0);
		metadata = inConn.getMetadataTable();
	}
	
	String user = null;
	if (metadata!=null) {    
%>
		String username_<%=cid%> = "";
		org.apache.hadoop.fs.FileSystem fs_<%=cid%> = null;
		<%
		if(!useExistingConnection) { // if we don't use an existing connection, we create a new hadoop configuration
			String hadoopVersion = ElementParameterParser.getValue(node, "__DB_VERSION__");
			boolean useKrb = "true".equals(ElementParameterParser.getValue(node, "__USE_KRB__"));
			String kerberosPrincipal = ElementParameterParser.getValue(node, "__NAMENODE_PRINCIPAL__");
    		
    		boolean isCustom = "CUSTOM".equals(ElementParameterParser.getValue(node, "__DISTRIBUTION__"));
			String auth = ElementParameterParser.getValue(node, "__AUTHENTICATION_MODE__");

			%>
			org.apache.hadoop.conf.Configuration conf_<%=cid%> = new org.apache.hadoop.conf.Configuration();
			conf_<%=cid%>.set("<%=fsDefalutName%>", <%=fsDefaultName%>);
			<%

    		if(!(((("HDP_1_0").equals(hadoopVersion) || ("HDP_1_2").equals(hadoopVersion) || ("HDP_1_3").equals(hadoopVersion) || ("APACHE_1_0_0").equals(hadoopVersion) || ("APACHE_1_0_3_EMR").equals(hadoopVersion) || ("Cloudera_CDH4").equals(hadoopVersion) && !isCustom) && useKrb)
    		 || (isCustom && "KRB".equals(auth)))) {
				user = ElementParameterParser.getValue(node, "__USERNAME__");
			} else {
%>
				conf_<%=cid%>.set("dfs.namenode.kerberos.principal", <%=kerberosPrincipal%>);
<%
			}
			if(hadoopProps.size() > 0){
				for(Map<String, String> item : hadoopProps){
				%>
					conf_<%=cid%>.set(<%=item.get("PROPERTY") %> ,<%=item.get("VALUE") %>);
				<% 
				} 
			}
			if(((("APACHE_0_20_2").equals(hadoopVersion) || ("MAPR1").equals(hadoopVersion) || ("MAPR2").equals(hadoopVersion) || ("MAPR212").equals(hadoopVersion) || ("MapR_EMR").equals(hadoopVersion)) && !isCustom) || (isCustom && "UGI".equals(auth))){
				String group = ElementParameterParser.getValue(node, "__GROUP__");
			%>
				conf_<%=cid%>.set("hadoop.job.ugi",<%=user%>+","+<%=group%>);
				fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(conf_<%=cid%>);
			<%
			}else{
			%>
				username_<%=cid%> = <%=user%>;
				if(username_<%=cid%> == null || "".equals(username_<%=cid%>)){
					fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(conf_<%=cid%>);
				}else{
					fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(new java.net.URI(conf_<%=cid%>.get("<%=fsDefalutName%>")),conf_<%=cid%>,username_<%=cid%>);
				}	
			<%
			}
		} else { // We re use the existing connection, coming from the component learned.
			String connectionSid = ElementParameterParser.getValue(node, "__CONNECTION__");
			%>
			org.apache.hadoop.conf.Configuration conf_<%=cid%> = (org.apache.hadoop.conf.Configuration)globalMap.get("conn_<%=connectionSid%>");
			<%
			List<? extends INode> nodes = node.getProcess().getGeneratingNodes();
		    for(INode targetNode : nodes){
		    	if (targetNode.getUniqueName().equals(connectionSid)) {
			    String hadoopVersion = ElementParameterParser.getValue(targetNode, "__DB_VERSION__");   	
				boolean useKrb = "true".equals(ElementParameterParser.getValue(targetNode, "__USE_KRB__"));
				String kerberosPrincipal = ElementParameterParser.getValue(targetNode, "__NAMENODE_PRINCIPAL__");
			
    			boolean isCustom = "CUSTOM".equals(ElementParameterParser.getValue(targetNode, "__DISTRIBUTION__"));
    			String auth = ElementParameterParser.getValue(targetNode, "__AUTHENTICATION_MODE__");
    			
    		      	if(((("APACHE_0_20_2").equals(hadoopVersion) || ("MAPR1").equals(hadoopVersion) || ("MAPR2").equals(hadoopVersion) || ("MAPR212").equals(hadoopVersion) || ("MapR_EMR").equals(hadoopVersion)) && !isCustom) || (isCustom && "UGI".equals(auth))){
				   	%>
						fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(conf_<%=cid%>);
					<%
				  	}else{
						if(!(((("HDP_1_0").equals(hadoopVersion) || ("HDP_1_2").equals(hadoopVersion) || ("HDP_1_3").equals(hadoopVersion) || ("APACHE_1_0_0").equals(hadoopVersion) || ("APACHE_1_0_3_EMR").equals(hadoopVersion) || ("Cloudera_CDH4").equals(hadoopVersion)) && !isCustom && useKrb) || (isCustom && "KRB".equals(auth)))) {
							user = ElementParameterParser.getValue(targetNode, "__USERNAME__");
						} else {
%>
							conf_<%=cid%>.set("dfs.namenode.kerberos.principal", <%=kerberosPrincipal%>);
<%
						}
				  	%>
				  		username_<%=cid%> = <%=user%>;
						if(username_<%=cid%> == null || "".equals(username_<%=cid%>)){
							fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(conf_<%=cid%>);
						}else{
							fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(new java.net.URI(conf_<%=cid%>.get("<%=fsDefalutName%>")),conf_<%=cid%>,username_<%=cid%>);
						}
				  	<%
				  	}
			      	break;
			    }
		    }
		}
		%>
		org.apache.hadoop.fs.Path path_<%=cid%> = new org.apache.hadoop.fs.Path(<%=fileName%>);

<%
	if (typeFile.equals("TEXT")) {
%>			
		org.apache.hadoop.fs.FSDataOutputStream fsDataOutputStream_<%=cid%> = null;
		<%if("APPEND".equals(fileAction)){%>
			fsDataOutputStream_<%=cid%> = fs_<%=cid%>.append(path_<%=cid%>);
		<%}else{//Create and Overwrite%>
			fsDataOutputStream_<%=cid%> = fs_<%=cid%>.create(path_<%=cid%>, <%="CREATE".equals(fileAction)?false:true%>);
		<%}%>
<%
		if(!compress) {
%>
			java.io.Writer out<%=cid%> = null;
			out<%=cid%>=new java.io.BufferedWriter(new java.io.OutputStreamWriter(fsDataOutputStream_<%=cid%><%if(customEncoding){%>,<%=encoding%><%}%>));
	<%
		} else {
			if("GZIP".equals(compression)) {
%>
				org.apache.hadoop.io.compress.GzipCodec codec_<%=cid%> = new org.apache.hadoop.io.compress.GzipCodec();
				codec_<%=cid%>.setConf(conf_<%=cid%>);
<%
			} else if("BZIP2".equals(compression)) {
%>
				org.apache.hadoop.io.compress.BZip2Codec codec_<%=cid%> = new org.apache.hadoop.io.compress.BZip2Codec(); 
<%
			}
%>
			org.apache.hadoop.io.compress.CompressionOutputStream out<%=cid%> = codec_<%=cid%>.createOutputStream(fsDataOutputStream_<%=cid%>);
<%
		}
		
		if(isIncludeHeader) {
			String rowSeparator = ElementParameterParser.getValue(node, "__ROWSEPARATOR__");
        	String fieldSeparator = ElementParameterParser.getValue(node, "__FIELDSEPARATOR__");
        	
			if("APPEND".equals(fileAction)){
%>
			boolean fileExistAndHasContent_<%=cid%> = false;
			if(fs_<%=cid%>.exists(path_<%=cid%>)) {
				org.apache.hadoop.fs.FileStatus statu_<%=cid%> = fs_<%=cid%>.getFileStatus(path_<%=cid%>);
				fileExistAndHasContent_<%=cid%> = (!statu_<%=cid%>.isDir()) && (statu_<%=cid%>.getLen() != 0);
			}
			if(!fileExistAndHasContent_<%=cid%>) {
<%
			}
%>
			StringBuilder header_<%=cid %> = new StringBuilder();
<%
    		List<IMetadataColumn> columns = metadata.getListColumns();
	        int sizeColumns = columns.size();
	        for (int i = 0; i < sizeColumns; i++) {
	        	IMetadataColumn column = columns.get(i);
%>
				header_<%=cid %>.append("<%=column.getLabel() %>");
<%
	            if(i != sizeColumns - 1) {
%>
                header_<%=cid %>.append(<%=fieldSeparator%>);
<%
	            }
	        }
%>
			header_<%=cid %>.append(<%=rowSeparator%>);
<%
			if(!compress) {
%>
			out<%=cid %>.write(header_<%=cid %>.toString());
<%
			} else {
%>
			out<%=cid%>.write(header_<%=cid %>.toString().getBytes(<%=customEncoding?encoding:""%>));
<%
			}
			if("APPEND".equals(fileAction)){
%>
			}
<%
			}
		}
	} else {
		String keyColumn = ElementParameterParser.getValue(node,"__KEYCOLUMN__");
		String valueColumn = ElementParameterParser.getValue(node,"__VALUECOLUMN__");
			
		List<IMetadataColumn> listColumns = metadata.getListColumns();
		String talendKeyClass = "";
		String talendValueClass = "";
		for (IMetadataColumn column : listColumns) {
			if (column.getLabel().equals(keyColumn)) {
				talendKeyClass = column.getTalendType();
			}
			if (column.getLabel().equals(valueColumn)) {
				talendValueClass = column.getTalendType();
			}
		}
			
		String keyClass="org.apache.hadoop.io.Text";
		if (talendKeyClass.equals("id_Boolean")) keyClass="org.apache.hadoop.io.BooleanWritable";
		if (talendKeyClass.equals("id_Byte")) keyClass="org.apache.hadoop.io.ByteWritable";
		if (talendKeyClass.equals("id_byte[]")) keyClass="org.apache.hadoop.io.BytesWritable";
		if (talendKeyClass.equals("id_Double")) keyClass="org.apache.hadoop.io.DoubleWritable";
		if (talendKeyClass.equals("id_Float")) keyClass="org.apache.hadoop.io.FloatWritable";
		if (talendKeyClass.equals("id_Integer")) keyClass="org.apache.hadoop.io.IntWritable";
		if (talendKeyClass.equals("id_Long")) keyClass="org.apache.hadoop.io.LongWritable";
		if (talendKeyClass.equals("id_Short")) keyClass="org.apache.hadoop.io.ShortWritable";
		if (talendKeyClass.equals("id_String")) keyClass="org.apache.hadoop.io.Text";
			
		String valueClass="org.apache.hadoop.io.Text";
		if (talendValueClass.equals("id_Boolean")) valueClass="org.apache.hadoop.io.BooleanWritable";
		if (talendValueClass.equals("id_Byte")) valueClass="org.apache.hadoop.io.ByteWritable";
		if (talendValueClass.equals("id_byte[]")) valueClass="org.apache.hadoop.io.BytesWritable";
		if (talendValueClass.equals("id_Double")) valueClass="org.apache.hadoop.io.DoubleWritable";
		if (talendValueClass.equals("id_Float")) valueClass="org.apache.hadoop.io.FloatWritable";
		if (talendValueClass.equals("id_Integer")) valueClass="org.apache.hadoop.io.IntWritable";
		if (talendValueClass.equals("id_Long")) valueClass="org.apache.hadoop.io.LongWritable";
		if (talendValueClass.equals("id_Short")) valueClass="org.apache.hadoop.io.ShortWritable";
		if (talendValueClass.equals("id_String")) valueClass="org.apache.hadoop.io.Text";
		
		if(!compress) {
%>
	org.apache.hadoop.io.SequenceFile.Writer writer_<%=cid%> = new org.apache.hadoop.io.SequenceFile.Writer(fs_<%=cid%>, conf_<%=cid%>, path_<%=cid%>, <%=keyClass%>.class, <%=valueClass%>.class);
	
<%
		} else {
			if("GZIP".equals(compression)) {
%>
	org.apache.hadoop.io.SequenceFile.Writer writer_<%=cid%> = org.apache.hadoop.io.SequenceFile.createWriter(fs_<%=cid%>, conf_<%=cid%>, path_<%=cid%>, <%=keyClass%>.class, <%=valueClass%>.class, org.apache.hadoop.io.SequenceFile.CompressionType.BLOCK, new org.apache.hadoop.io.compress.GzipCodec());
<%
			} else if("BZIP2".equals(compression)) {
%>
	org.apache.hadoop.io.SequenceFile.Writer writer_<%=cid%> = org.apache.hadoop.io.SequenceFile.createWriter(fs_<%=cid%>, conf_<%=cid%>, path_<%=cid%>, <%=keyClass%>.class, <%=valueClass%>.class, org.apache.hadoop.io.SequenceFile.CompressionType.BLOCK, new org.apache.hadoop.io.compress.BZip2Codec());
<%
			}
		}
	}
	
	if(node.isVirtualGenerateNode() && metadata!=null){
		if (inConn.getLineStyle().hasConnectionCategory(IConnectionCategory.DATA)) {
			String origin = ElementParameterParser.getValue(node, "__DESTINATION__");
			cid = origin;
			String con_name = org.talend.core.model.utils.NodeUtil.getPrivateConnClassName(inConn);
%>
			List<<%=con_name%>Struct> <%=con_name%>_list_<%=cid%>= new java.util.ArrayList<<%=con_name%>Struct>();
<%
		}
	}
}
%>
