<%@ jet 
	imports="
    org.talend.core.model.process.INode 
    org.talend.core.model.metadata.IMetadataTable 
	org.talend.core.model.metadata.IMetadataColumn 
    org.talend.designer.codegen.config.CodeGeneratorArgument
	org.talend.core.model.process.ElementParameterParser
	java.util.Map 
    java.util.List 
	"
%>

<%
	CodeGeneratorArgument codeGenArgument = (CodeGeneratorArgument) argument;
	INode node = (INode)codeGenArgument.getArgument();
	String cid = node.getUniqueName();
	
	String fsDefaultName = ElementParameterParser.getValue(node, "__FS_DEFAULT_NAME__");
	String mapredJobTracker = ElementParameterParser.getValue(node, "__MAPRED_JOB_TRACKER__");
	
	boolean useExistingConnection = "true".equals(ElementParameterParser.getValue(node, "__USE_EXISTING_CONNECTION__"));
	List<Map<String, String>> hadoopProps = (List<Map<String,String>>)ElementParameterParser.getObjectValue(node, "__HADOOP_ADVANCED_PROPERTIES__");
		
	String fileName = ElementParameterParser.getValue(node, "__FILENAME__");
	
	String fileAction = ElementParameterParser.getValue(node, "__FILE_ACTION__");
	
	String rowSeparator = ElementParameterParser.getValue(node, "__ROWSEPARATOR__");
	String fieldSeparator = ElementParameterParser.getValue(node, "__FIELDSEPARATOR__");
	boolean customEncoding="true".equals( ElementParameterParser.getValue(node,"__CUSTOM_ENCODING__"));
	String encoding = ElementParameterParser.getValue(node,"__ENCODING__");
	
	String typeFile = ElementParameterParser.getValue(node,"__TYPEFILE__");
	
	boolean uncompress = "true".equals(ElementParameterParser.getValue(node, "__UNCOMPRESS__"));
	String compression = ElementParameterParser.getValue(node, "__COMPRESSION__");
	
	IMetadataTable metadata = null;
	
	String user = null;
	List<IMetadataTable> metadatas = node.getMetadataList();
	if ((metadatas!=null)&&(metadatas.size()>0)) {
		metadata = metadatas.get(0);
		if (metadata!=null) { 
	%>
		String username_<%=cid%> = "";
		org.apache.hadoop.fs.FileSystem fs_<%=cid%> = null;
		<%   
		if(!useExistingConnection) { // if we don't use an existing connection, we create a new hadoop configuration
			String hadoopVersion = ElementParameterParser.getValue(node, "__DB_VERSION__");
			boolean useKrb = "true".equals(ElementParameterParser.getValue(node, "__USE_KRB__"));
			String kerberosPrincipal = ElementParameterParser.getValue(node, "__NAMENODE_PRINCIPAL__");
    		boolean isCustom = "CUSTOM".equals(ElementParameterParser.getValue(node, "__DISTRIBUTION__"));
    		String auth = ElementParameterParser.getValue(node, "__AUTHENTICATION_MODE__");
			%>
			org.apache.hadoop.conf.Configuration conf_<%=cid%> = new org.apache.hadoop.conf.Configuration();
			conf_<%=cid%>.set("fs.default.name", <%=fsDefaultName%>);
			<%
			if(!(((("HDP_1_0").equals(hadoopVersion) || ("HDP_1_2").equals(hadoopVersion) || ("APACHE_1_0_0").equals(hadoopVersion) || ("APACHE_1_0_3_EMR").equals(hadoopVersion) || ("Cloudera_CDH4").equals(hadoopVersion) && !isCustom) && useKrb)
				 || (isCustom && "KRB".equals(auth)))) {
				user = ElementParameterParser.getValue(node, "__USERNAME__");
			} else {
%>
				conf_<%=cid%>.set("dfs.namenode.kerberos.principal", <%=kerberosPrincipal%>);
<%
			}
			if(hadoopProps.size() > 0){
				for(Map<String, String> item : hadoopProps){
				%>
					conf_<%=cid%>.set(<%=item.get("PROPERTY") %> ,<%=item.get("VALUE") %>);
				<% 
				} 
			}
			if(((("APACHE_0_20_2").equals(hadoopVersion) || ("MapR").equals(hadoopVersion) || ("MapR_EMR").equals(hadoopVersion)) && !isCustom) || (isCustom && "UGI".equals(auth))){
				String group = ElementParameterParser.getValue(node, "__GROUP__");
				%>
				conf_<%=cid%>.set("hadoop.job.ugi",<%=user%>+","+<%=group%>);
				fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(conf_<%=cid%>);
			<%
			}else{
			%>
				username_<%=cid%> = <%=user%>;
				if(username_<%=cid%> == null || "".equals(username_<%=cid%>)){
					fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(conf_<%=cid%>);
				}else{
					fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(new java.net.URI(conf_<%=cid%>.get("fs.default.name")),conf_<%=cid%>,username_<%=cid%>);
				}	
			<%
			}
			
		} else { // We re use the existing connection, coming from the component learned.
			String connectionSid = ElementParameterParser.getValue(node, "__CONNECTION__");
			%>
			org.apache.hadoop.conf.Configuration conf_<%=cid%> = (org.apache.hadoop.conf.Configuration)globalMap.get("conn_<%=connectionSid%>");
			<%
			List<? extends INode> nodes = node.getProcess().getGeneratingNodes();
		    for(INode targetNode : nodes){
		    	if (targetNode.getUniqueName().equals(connectionSid)) {
			      	String hadoopVersion = ElementParameterParser.getValue(targetNode, "__DB_VERSION__");
				boolean useKrb = "true".equals(ElementParameterParser.getValue(targetNode, "__USE_KRB__"));
				String kerberosPrincipal = ElementParameterParser.getValue(targetNode, "__NAMENODE_PRINCIPAL__");
			
    			boolean isCustom = "CUSTOM".equals(ElementParameterParser.getValue(targetNode, "__DISTRIBUTION__"));
    			String auth = ElementParameterParser.getValue(targetNode, "__AUTHENTICATION_MODE__");
    			
    		      	if(((("APACHE_0_20_2").equals(hadoopVersion) || ("MapR").equals(hadoopVersion) || ("MapR_EMR").equals(hadoopVersion)) && !isCustom) || (isCustom && "UGI".equals(auth))){
				    %>
				    	fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(conf_<%=cid%>);
					<%
				  	}else{
					if(!(((("HDP_1_0").equals(hadoopVersion) || ("HDP_1_2").equals(hadoopVersion) || ("APACHE_1_0_0").equals(hadoopVersion) || ("APACHE_1_0_3_EMR").equals(hadoopVersion) || ("Cloudera_CDH4").equals(hadoopVersion)) && !isCustom && useKrb) || (isCustom && "KRB".equals(auth)))) {
							user = ElementParameterParser.getValue(targetNode, "__USERNAME__");
						} else {
%>
							conf_<%=cid%>.set("dfs.namenode.kerberos.principal", <%=kerberosPrincipal%>);
<%
						}
				  	%>
				  		username_<%=cid%> = <%=user%>;
						if(username_<%=cid%> == null || "".equals(username_<%=cid%>)){
							fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(conf_<%=cid%>);
						}else{
							fs_<%=cid%> = org.apache.hadoop.fs.FileSystem.get(new java.net.URI(conf_<%=cid%>.get("fs.default.name")),conf_<%=cid%>,username_<%=cid%>);
						}
				  	<%
				  	}
			      	break;
			    }
		    }
		}
%>
		org.apache.hadoop.fs.Path path_<%=cid%> = new org.apache.hadoop.fs.Path(<%=fileName%>);
<%
		if (typeFile.equals("TEXT")) {
%>
		org.apache.hadoop.fs.FSDataInputStream fsDataInputStream_<%=cid%> = fs_<%=cid%>.open(path_<%=cid%>);
<%
			if(!uncompress) {
%>
		org.talend.fileprocess.FileInputDelimited fid_<%=cid %> = new org.talend.fileprocess.FileInputDelimited(fsDataInputStream_<%=cid%>,  <%=(customEncoding?encoding:null) %>,<%=fieldSeparator %>,<%=rowSeparator %>,false,0,0,-1,-1, false);
<%
			} else {
				if("GZIP".equals(compression)) {
%>
				org.apache.hadoop.io.compress.GzipCodec codec_<%=cid%> = new org.apache.hadoop.io.compress.GzipCodec();
				codec_<%=cid%>.setConf(conf_<%=cid%>);				
<%
				} else if("BZIP2".equals(compression)) {
%>
				org.apache.hadoop.io.compress.BZip2Codec codec_<%=cid%> = new org.apache.hadoop.io.compress.BZip2Codec(); 
<%
				}
%>
			org.apache.hadoop.io.compress.CompressionInputStream in<%=cid%> = codec_<%=cid%>.createInputStream(fsDataInputStream_<%=cid%>);
			org.talend.fileprocess.FileInputDelimited fid_<%=cid %> = new org.talend.fileprocess.FileInputDelimited(in<%=cid%>,  <%=(customEncoding?encoding:null) %>,<%=fieldSeparator %>,<%=rowSeparator %>,false,0,0,-1,-1, false);
<%
			}
%>
		while (fid_<%=cid %>.nextRecord()) {
<%
		} else {
			String keyColumn = ElementParameterParser.getValue(node,"__KEYCOLUMN__");
			String valueColumn = ElementParameterParser.getValue(node,"__VALUECOLUMN__");
			
			List<IMetadataColumn> listColumns = metadata.getListColumns();
			String talendKeyClass = "";
			String talendValueClass = "";
			for (IMetadataColumn column : listColumns) {
				if (column.getLabel().equals(keyColumn)) {
					talendKeyClass = column.getTalendType();
				}
				if (column.getLabel().equals(valueColumn)) {
					talendValueClass = column.getTalendType();
				}
			}
			
			String keyClass="org.apache.hadoop.io.Text";
			if (talendKeyClass.equals("id_Boolean")) keyClass="org.apache.hadoop.io.BooleanWritable";
			if (talendKeyClass.equals("id_Byte")) keyClass="org.apache.hadoop.io.ByteWritable";
			if (talendKeyClass.equals("id_byte[]")) keyClass="org.apache.hadoop.io.BytesWritable";
			if (talendKeyClass.equals("id_Double")) keyClass="org.apache.hadoop.io.DoubleWritable";
			if (talendKeyClass.equals("id_Float")) keyClass="org.apache.hadoop.io.FloatWritable";
			if (talendKeyClass.equals("id_Integer")) keyClass="org.apache.hadoop.io.IntWritable";
			if (talendKeyClass.equals("id_Long")) keyClass="org.apache.hadoop.io.LongWritable";
			if (talendKeyClass.equals("id_Short")) keyClass="org.apache.hadoop.io.ShortWritable";
			if (talendKeyClass.equals("id_String")) keyClass="org.apache.hadoop.io.Text";
			
			String valueClass="org.apache.hadoop.io.Text";
			if (talendValueClass.equals("id_Boolean")) valueClass="org.apache.hadoop.io.BooleanWritable";
			if (talendValueClass.equals("id_Byte")) valueClass="org.apache.hadoop.io.ByteWritable";
			if (talendValueClass.equals("id_byte[]")) valueClass="org.apache.hadoop.io.BytesWritable";
			if (talendValueClass.equals("id_Double")) valueClass="org.apache.hadoop.io.DoubleWritable";
			if (talendValueClass.equals("id_Float")) valueClass="org.apache.hadoop.io.FloatWritable";
			if (talendValueClass.equals("id_Integer")) valueClass="org.apache.hadoop.io.IntWritable";
			if (talendValueClass.equals("id_Long")) valueClass="org.apache.hadoop.io.LongWritable";
			if (talendValueClass.equals("id_Short")) valueClass="org.apache.hadoop.io.ShortWritable";
			if (talendValueClass.equals("id_String")) valueClass="org.apache.hadoop.io.Text";
			
%>
		org.apache.hadoop.io.SequenceFile.Reader reader_<%=cid%> = new org.apache.hadoop.io.SequenceFile.Reader(fs_<%=cid%>, path_<%=cid%>,  conf_<%=cid%>);
		<%=keyClass%> key_<%=cid%> = (<%=keyClass%>) reader_<%=cid%>.getKeyClass().newInstance();
		<%=valueClass%> value_<%=cid%> = (<%=valueClass%>) reader_<%=cid%>.getValueClass().newInstance();
		while (reader_<%=cid%>.next(key_<%=cid%>, value_<%=cid%>)) {
<%
		}
%>
		
<%
		}
	}
%>